version: '2'
volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
services:

  mysql:
    image: mysql:latest
    container_name: c-mysql
    restart: always
    hostname: mysql
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=123
    volumes:
      - ./mydata:/var/lib/mysql
    networks:
      - bigdata
      
  hadoop-master:
    container_name: hadoop-master
    image: nguyennam250303/hadoop-docker:1.0
    volumes:
      - namenode_data:/root/hdfs/namenode
    restart: always
    ports:
      - "50070:50070"
      - "9000:9000"
      - "8088:8088"
    networks:
      - bigdata
  datanode1:
    container_name: hadoop-slave1
    image: nguyennam250303/hadoop-docker:1.0
    hostname: hadoop-slave1
    volumes:
      - datanode1_data:/root/hdfs/datanode
    restart: always
    depends_on:
      - hadoop-master
    networks:
      - bigdata
  datanode2:
    container_name: hadoop-slave2
    image: nguyennam250303/hadoop-docker:1.0
    hostname: hadoop-slave2
    volumes:
      - datanode2_data:/root/hdfs/datanode
    restart: always
    depends_on:
      - hadoop-master
    networks:
      - bigdata
  spark-master:
    container_name: spark-master
    image: bitnami/spark:latest
    restart: always
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_DRIVER_MEMORY=2G
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-master:9000
    volumes:
      - ./spark/mycode:/tmp/mycode/
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark/conf/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./drivers/mysql-connector-j-8.0.33.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar
    ports:
      - '8090:8080'
    networks:
      - bigdata
  spark-worker1:
    container_name: spark-worker1
    image: bitnami/spark:latest
    restart: always
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - bigdata
  
  spark-notebook:
    container_name: spark-notebook
    build:
      context: ./spark-notebook/.
      dockerfile: Dockerfile
    user: root
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - GRANT_SUDO="yes"
    volumes:
      - ./spark-notebook:/home/jovyan/work
      - ./spark-notebook/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
      - ./spark-notebook/hive-site.xml:/usr/local/spark/conf/hive-site.xml
    ports:
      - "8888:8888"
      - "4040:4040"
    networks:
      - bigdata

  hive-server:
    container_name: hive-server
    build:
      dockerfile: Dockerfile
      context: ./hive-docker/
    ports:
      - "10002:10002"
      - "9083:9083"
    restart: always
    depends_on:
      - hadoop-master
      - datanode1
      - datanode2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-master:9000
    networks:
      - bigdata


networks:
  bigdata:
    external: true

