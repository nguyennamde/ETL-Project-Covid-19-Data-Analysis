{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d63ead3d-15ee-4d89-b0b5-400e18aacae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, date_format, year, month, dayofmonth, concat, lower\n",
    "import datetime as dt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c17b5e7-a989-45b0-b091-9375eeef3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.ERROR,\n",
    "                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s')\n",
    "def createSparkSession():\n",
    "    try:\n",
    "        spark = SparkSession.\\\n",
    "                            builder.\\\n",
    "                            appName(\"Spark ETL\").\\\n",
    "                            master(\"spark://spark-master:7077\").\\\n",
    "                            config(\"spark.jars\", \"mysql-connector-j-8.0.33.jar\").\\\n",
    "                            config(\"hive.metastore.uris\", \"thrift://hive-server:9083\").\\\n",
    "                            enableHiveSupport().\\\n",
    "                            getOrCreate()\n",
    "        logging.info(\"SparkSession was be created successfully\")\n",
    "    except Exception:\n",
    "        logging.error(\"Fail to create SparkSession\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7546eb32-2fc3-4267-80f9-993b5e49d863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 16:11:44,571:createSparkSession:INFO:SparkSession was be created successfully\n"
     ]
    }
   ],
   "source": [
    "spark = createSparkSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf806c47-a5e6-4b4b-8127-ea8edf679ebe",
   "metadata": {},
   "source": [
    "## EL MySQL to Datalake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd5c65-034c-4299-9ce2-8a290a827b25",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b61fc57-4f8c-4281-b156-03f96047e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_covid19_timeseries_mysql(spark):\n",
    "    try:\n",
    "        df_covid19_timeseries = spark.read \\\n",
    "                                     .format(\"jdbc\") \\\n",
    "                                     .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "                                     .option(\"url\", \"jdbc:mysql://c-mysql:3306/covid19\") \\\n",
    "                                     .option(\"dbtable\", \"covid19_timeseries\") \\\n",
    "                                     .option(\"user\", \"root\") \\\n",
    "                                     .option(\"password\", \"123\") \\\n",
    "                                     .load()\n",
    "        logging.info(\"Read covid19_timeseries from mysql successfully\")\n",
    "    except Exception:\n",
    "        logging.warning(\"Couldn't read covid19_timeseries from mysql\")\n",
    "        \n",
    "    return df_covid19_timeseries\n",
    "def extract_worldometer_mysql(spark):\n",
    "    try:\n",
    "        df_worldometer = spark.read \\\n",
    "                              .format(\"jdbc\") \\\n",
    "                              .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "                              .option(\"url\", \"jdbc:mysql://c-mysql:3306/covid19\") \\\n",
    "                              .option(\"dbtable\", \"worldometer\") \\\n",
    "                              .option(\"user\", \"root\") \\\n",
    "                              .option(\"password\", \"123\") \\\n",
    "                              .load()\n",
    "        logging.info(\"Read worldometer from mysql successfully\")\n",
    "    except Exception:\n",
    "        logging.warning(\"Couldn't read worldometer from mysql\")\n",
    "    return df_worldometer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e101b850-b2b9-450a-8c7c-f7f3177fc317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 16:48:11,373:extract_covid19_timeseries_mysql:INFO:Read covid19_timeseries from mysql successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+---------+------+---------+------+--------------------+----+\n",
      "|    country|    lat_|  long_|               date|confirmed|deaths|recovered|active|          who_region|uuid|\n",
      "+-----------+--------+-------+-------------------+---------+------+---------+------+--------------------+----+\n",
      "|Afghanistan| 33.9391|  67.71|2020-01-22 00:00:00|        0|     0|        0|     0|Eastern Mediterra...|   1|\n",
      "|    Albania| 41.1533|20.1683|2020-01-22 00:00:00|        0|     0|        0|     0|              Europe|   2|\n",
      "|    Algeria| 28.0339| 1.6596|2020-01-22 00:00:00|        0|     0|        0|     0|              Africa|   3|\n",
      "|    Andorra| 42.5063| 1.5218|2020-01-22 00:00:00|        0|     0|        0|     0|              Europe|   4|\n",
      "|     Angola|-11.2027|17.8739|2020-01-22 00:00:00|        0|     0|        0|     0|              Africa|   5|\n",
      "+-----------+--------+-------+-------------------+---------+------+---------+------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_covid19_timeseries_mysql(spark).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b4d1d02-bc1d-4e81-82c2-09e6d827fe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 16:48:13,671:extract_worldometer_mysql:INFO:Read worldometer from mysql successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------+----------------+---------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------+-------+-----------+---------------------------+\n",
      "|rank|cca3|       country|         capital|continent|2022_population|2020_population|2015_population|2010_population|2000_population|1990_population|1980_population|1970_population|     area|density|growth_rate|world_population_percentage|\n",
      "+----+----+--------------+----------------+---------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------+-------+-----------+---------------------------+\n",
      "|  36| AFG|   Afghanistan|           Kabul|     Asia|       41128771|       38972230|       33753499|       28189672|       19542982|       10694796|       12486631|       10752971| 652230.0|63.0587|     1.0257|                       0.52|\n",
      "| 138| ALB|       Albania|          Tirana|   Europe|        2842321|        2866849|        2882481|        2913399|        3182021|        3295066|        2941651|        2324731|  28748.0|98.8702|     0.9957|                       0.04|\n",
      "|  34| DZA|       Algeria|         Algiers|   Africa|       44903225|       43451666|       39543154|       35856344|       30774621|       25518074|       18739378|       13795915|2381740.0|18.8531|     1.0164|                       0.56|\n",
      "| 213| ASM|American Samoa|       Pago Pago|  Oceania|          44273|          46189|          51368|          54849|          58230|          47818|          32886|          27075|    199.0|222.477|     0.9831|                        0.0|\n",
      "| 203| AND|       Andorra|Andorra la Vella|   Europe|          79824|          77700|          71746|          71519|          66097|          53569|          35611|          19860|    468.0|170.564|       1.01|                        0.0|\n",
      "+----+----+--------------+----------------+---------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------+-------+-----------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_worldometer_mysql(spark).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c45407-a119-4dec-a94c-85488d360a59",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bad51c8-4338-4b2d-8cfa-d40bfbe56c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_datalake(raw_data, name_table):\n",
    "    try:\n",
    "        raw_data.write \\\n",
    "         .format(\"parquet\") \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .save(f\"hdfs://hadoop-master:9000/datalake/{name_table}\")\n",
    "        logging.info(f\"load {name_table} to datalake successfully\")\n",
    "    except Exception:\n",
    "        logging.warning(f\"Couldn't load {name_table} to datalake\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d3033a8-14f7-477f-a593-b60f8a3518b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 16:48:13,902:extract_covid19_timeseries_mysql:INFO:Read covid19_timeseries from mysql successfully\n",
      "2023-10-01 16:48:15,423:load_to_datalake:INFO:load covid19_timeseries to datalake successfully\n",
      "2023-10-01 16:48:15,450:extract_worldometer_mysql:INFO:Read worldometer from mysql successfully\n",
      "2023-10-01 16:48:15,712:load_to_datalake:INFO:load worldometer to datalake successfully\n"
     ]
    }
   ],
   "source": [
    "load_to_datalake(extract_covid19_timeseries_mysql(spark) \\\n",
    "                                    , \"covid19_timeseries\")\n",
    "load_to_datalake(extract_worldometer_mysql(spark) \\\n",
    "                                    , \"worldometer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e8b5d-19b6-4725-9e68-1723053a5f8c",
   "metadata": {},
   "source": [
    "## ETL Datalake to Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a865d8-2f5a-4dd0-a0d7-3d96a6af2500",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2edc0394-fe83-44ea-b461-2d661a126e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datalake(spark, name_table):\n",
    "    try:\n",
    "        df = spark.read \\\n",
    "                  .format(\"parquet\") \\\n",
    "                  .load(f\"hdfs://hadoop-master:9000/datalake/{name_table}\")\n",
    "        logging.info(f\"Read {name_table} from datalake successfully\")\n",
    "    except Exception:\n",
    "        logging.warning(f\"Couldn't read {name_table} from datalake\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58b49c08-9622-47ef-9f97-b7b2d6424b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 16:48:15,852:extract_datalake:INFO:Read covid19_timeseries from datalake successfully\n",
      "2023-10-01 16:48:15,951:extract_datalake:INFO:Read worldometer from datalake successfully\n"
     ]
    }
   ],
   "source": [
    "df_covid19_timeseries = extract_datalake(spark, \"covid19_timeseries\")\n",
    "df_worldometer = extract_datalake(spark, \"worldometer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243eda8-dd86-4cad-b7f9-dcc348cee66f",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "108e27e7-1fd9-4237-a7d0-97316a68ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df_covid19_timeseries, df_worldometer):\n",
    "    cleaned_worldometer = df_worldometer.select(col(\"country\") \\\n",
    "                                           , col(\"continent\") \\\n",
    "                                           , col(\"2020_population\"))\n",
    "\n",
    "    \n",
    "    cleaned_covid19_timeseries = df_covid19_timeseries \\\n",
    "                            .filter((col(\"confirmed\") != 0) \\\n",
    "                                    | (col(\"deaths\") != 0) \\\n",
    "                                    | (col(\"active\") != 0))\n",
    "    cleaned_covid19_timeseries = cleaned_covid19_timeseries.withColumn(\"date\", col(\"date\") \\\n",
    "                                           .cast(\"date\"))\n",
    "    cleaned_covid19_timeseries = cleaned_covid19_timeseries.withColumn(\"date_id\" \\\n",
    "                                           , date_format(col(\"date\"), \"yyyyMMdd\"))\n",
    "    cleaned_covid19_timeseries = cleaned_covid19_timeseries.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "                               .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "                               .withColumn(\"day\", dayofmonth(col(\"date\")))\n",
    "    cleaned_covid19_timeseries = cleaned_covid19_timeseries.drop(\"date\")\n",
    "    cleaned_covid19_timeseries = cleaned_covid19_timeseries.withColumn(\"who_id\", lower(col(\"who_region\")).substr(0,3))\n",
    "    \n",
    "    cleaned_data = cleaned_covid19_timeseries.join(cleaned_worldometer, \"country\", \"inner\")\n",
    "    cleaned_data = cleaned_data.withColumn(\"pop_loc_id\", lower(concat(col(\"country\").substr(0,2) \\\n",
    "                           , col(\"lat_\").cast(\"int\") \\\n",
    "                           , col(\"long_\").cast(\"int\"))))\n",
    "    return cleaned_data\n",
    "def transform_worldometer(df_worldometer):\n",
    "    cleaned_data = df_worldometer.select(col(\"country\") \\\n",
    "                                        , col(\"continent\") \\\n",
    "                                        , col(\"population\"))\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bba326b-8fb3-40e2-97a8-cbf2c09c5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = transform(df_covid19_timeseries, df_worldometer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7399c0-4ae6-46d3-97f6-80c1d7e7e321",
   "metadata": {},
   "source": [
    "#### Dimesional data modeling with star schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c08f0993-567c-491e-a9f1-c92e4bdc7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fact_table(cleaned_data):\n",
    "    fact_covid = cleaned_data.select(col(\"uuid\") \\\n",
    "                                    , col(\"date_id\") \\\n",
    "                                    , col(\"who_id\") \\\n",
    "                                    , col(\"pop_loc_id\") \\\n",
    "                                    , col(\"confirmed\") \\\n",
    "                                    , col(\"deaths\") \\\n",
    "                                    , col(\"recovered\") \\\n",
    "                                    , col(\"active\"))\n",
    "    return fact_covid\n",
    "def create_dimensional_table(cleaned_data):\n",
    "    dim_date = cleaned_data.select(col(\"date_id\") \\\n",
    "                                     , col(\"year\") \\\n",
    "                                     , col(\"month\") \\\n",
    "                                     , col(\"day\")) \\\n",
    "                        .dropDuplicates()\n",
    "    \n",
    "    dim_who_region = cleaned_data.select(col(\"who_id\") \\\n",
    "                                  , col(\"who_region\")) \\\n",
    "                             .dropDuplicates()\n",
    "    \n",
    "    dim_location = cleaned_data.select(col(\"pop_loc_id\") \\\n",
    "                                  , col(\"country\") \\\n",
    "                                  , col(\"continent\") \\\n",
    "                                  , col(\"lat_\") \\\n",
    "                                  , col(\"long_\")) \\\n",
    "                             .dropDuplicates()\n",
    "    \n",
    "    dim_population = cleaned_data.select(col(\"pop_loc_id\") \\\n",
    "                                  , col(\"2020_population\")) \\\n",
    "                             .dropDuplicates()\n",
    "    \n",
    "    return dim_date, dim_who_region, dim_location, dim_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a63d9e6-413e-42eb-a43f-a914714f73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_covid = create_fact_table(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d529620a-6474-43f8-b29a-58112941a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date, dim_who_region, dim_location, dim_population = create_dimensional_table(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "697faca4-c509-466c-a5c4-757df9e9caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+----------+---------+------+---------+------+\n",
      "|uuid| date_id|who_id|pop_loc_id|confirmed|deaths|recovered|active|\n",
      "+----+--------+------+----------+---------+------+---------+------+\n",
      "|  49|20200122|   wes|   ch31117|        1|     0|        0|     1|\n",
      "|  50|20200122|   wes|   ch40116|       14|     0|        0|    14|\n",
      "|  51|20200122|   wes|   ch30107|        6|     0|        0|     6|\n",
      "|  52|20200122|   wes|   ch26117|        1|     0|        0|     1|\n",
      "|  54|20200122|   wes|   ch23113|       26|     0|        0|    26|\n",
      "+----+--------+------+----------+---------+------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_covid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff7ca7b1-ff5f-45da-b024-4d8f44bf7bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+---+\n",
      "| date_id|year|month|day|\n",
      "+--------+----+-----+---+\n",
      "|20200127|2020|    1| 27|\n",
      "|20200522|2020|    5| 22|\n",
      "|20200311|2020|    3| 11|\n",
      "|20200420|2020|    4| 20|\n",
      "|20200228|2020|    2| 28|\n",
      "+--------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "685f0872-7acb-4f97-91ec-e29e19aad663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|who_id|          who_region|\n",
      "+------+--------------------+\n",
      "|   eas|Eastern Mediterra...|\n",
      "|   ame|            Americas|\n",
      "|   afr|              Africa|\n",
      "|   eur|              Europe|\n",
      "|   sou|     South-East Asia|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_who_region.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c23d694e-72d3-4551-a664-11e07509d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+--------+--------+\n",
      "|pop_loc_id|    country|    continent|    lat_|   long_|\n",
      "+----------+-----------+-------------+--------+--------+\n",
      "|  au-33151|  Australia|      Oceania|-33.8688| 151.209|\n",
      "|    es5825|    Estonia|       Europe| 58.5953| 25.0136|\n",
      "|  ca53-127|     Canada|North America| 53.7267|-127.648|\n",
      "|    fr3-53|     France|       Europe|  3.9339|-53.1258|\n",
      "|   sa13-60|Saint Lucia|North America| 13.9094|-60.9789|\n",
      "+----------+-----------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_location.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9336c6f-f9ad-4fb8-82f1-b35920c46159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|pop_loc_id|2020_population|\n",
      "+----------+---------------+\n",
      "|   ch40116|     1424929781|\n",
      "|  au-33151|       25670051|\n",
      "|    gu4-58|         797202|\n",
      "|   fr14-61|       64480053|\n",
      "|   ch37112|     1424929781|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_population.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "756f4e2f-5bf0-4316-8acc-88855a1a1ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d677e47f-0b96-4399-8e67-f95000d78155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_warehouse(fact_covid, dim_date, dim_who_region, dim_location, dim_population):\n",
    "    fact_covid = fact_covid.repartition(3)\n",
    "    dim_date = dim_date.repartition(3)\n",
    "    dim_who_region = dim_who_region.repartition(3)\n",
    "    dim_location = dim_location.repartition(3)\n",
    "    dim_population = dim_population.repartition(3)\n",
    "    spark.sql(\"create database covid19;\")\n",
    "    fact_covid.write \\\n",
    "              .format(\"hive\") \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .saveAsTable(\"covid19.fact_covid\")\n",
    "    dim_date.write \\\n",
    "              .format(\"hive\") \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .saveAsTable(\"covid19.dim_date\")\n",
    "    dim_who_region.write \\\n",
    "              .format(\"hive\") \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .saveAsTable(\"covid19.dim_who_region\")\n",
    "    dim_location.write \\\n",
    "              .format(\"hive\") \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .saveAsTable(\"covid19.dim_location\")\n",
    "    dim_population.write \\\n",
    "              .format(\"hive\") \\\n",
    "              .mode(\"overwrite\") \\\n",
    "              .saveAsTable(\"covid19.dim_population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63340863-3071-4e99-a9aa-7ca734d63835",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_warehouse(fact_covid, dim_date, dim_who_region, dim_location, dim_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9567b263-6c8f-4e49-ab4e-37625cbd21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 16:51:24,100:extract_covid19_timeseries_mysql:INFO:Read covid19_timeseries from mysql successfully\n",
      "2023-10-01 16:51:24,144:extract_worldometer_mysql:INFO:Read worldometer from mysql successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from mysql...\n",
      "Loading raw data to datalake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 16:51:25,429:load_to_datalake:INFO:load covid19_timeseries to datalake successfully\n",
      "2023-10-01 16:51:25,694:load_to_datalake:INFO:load worldometer to datalake successfully\n",
      "2023-10-01 16:51:25,785:extract_datalake:INFO:Read covid19_timeseries from datalake successfully\n",
      "2023-10-01 16:51:25,862:extract_datalake:INFO:Read worldometer from datalake successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ETL process...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o640.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 40) (172.18.0.14 executor 9): org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:274)\n\tat org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:148)\n\tat org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:81)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:286)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:271)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile.saveAsHiveFile(SaveAsHiveFile.scala:50)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile.saveAsHiveFile$(SaveAsHiveFile.scala:34)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:71)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:143)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:105)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:274)\n\tat org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:148)\n\tat org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:81)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:286)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:271)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [76], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m fact_covid \u001b[38;5;241m=\u001b[39m create_fact_table(cleaned_data)\n\u001b[1;32m     12\u001b[0m dim_date, dim_who_region, dim_location, dim_population \u001b[38;5;241m=\u001b[39m create_dimensional_table(cleaned_data)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mload_to_warehouse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfact_covid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_who_region\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_population\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinish ETL Process\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [74], line 8\u001b[0m, in \u001b[0;36mload_to_warehouse\u001b[0;34m(fact_covid, dim_date, dim_who_region, dim_location, dim_population)\u001b[0m\n\u001b[1;32m      6\u001b[0m dim_population \u001b[38;5;241m=\u001b[39m dim_population\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate database covid19;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mfact_covid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcovid19.fact_covid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m dim_date\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m     13\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m           \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m           \u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovid19.dim_date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m dim_who_region\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m     17\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m           \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m           \u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovid19.dim_who_region\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o640.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 40) (172.18.0.14 executor 9): org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:274)\n\tat org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:148)\n\tat org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:81)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:286)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:271)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile.saveAsHiveFile(SaveAsHiveFile.scala:50)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile.saveAsHiveFile$(SaveAsHiveFile.scala:34)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:71)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:143)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:105)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:274)\n\tat org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:148)\n\tat org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/jovyan/work/Notebook/spark-warehouse/covid19.db/fact_covid/.hive-staging_hive_2023-10-01_16-51-29_476_7586316276415486649-1/-ext-10000/_temporary/0/_temporary/attempt_202310011651316542452005649331396_0039_m_000002_40 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20230929025056-0002/9)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:81)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:286)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:271)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting data from mysql...\")\n",
    "df_covid19_timeseries = extract_covid19_timeseries_mysql(spark)\n",
    "df_worldometer = extract_worldometer_mysql(spark)\n",
    "print(\"Loading raw data to datalake...\")\n",
    "load_to_datalake(df_covid19_timeseries, \"covid19_timeseries\")\n",
    "load_to_datalake(df_worldometer, \"worldometer\")\n",
    "print(\"Running ETL process...\")\n",
    "raw_covid19_timeseries = extract_datalake(spark, \"covid19_timeseries\")\n",
    "raw_worldometer = extract_datalake(spark, \"worldometer\")\n",
    "cleaned_data = transform(raw_covid19_timeseries, raw_worldometer)\n",
    "fact_covid = create_fact_table(cleaned_data)\n",
    "dim_date, dim_who_region, dim_location, dim_population = create_dimensional_table(cleaned_data)\n",
    "load_to_warehouse(fact_covid, dim_date, dim_who_region, dim_location, dim_population)\n",
    "print(\"Finish ETL Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7155b1-90da-4038-ad98-ec45b62ecaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b2dc244-b19d-464d-806b-09d9da69c6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
